{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqWWpPYmdEkW",
        "outputId": "8f15b58b-f782-4f19-fcb9-4db4234191fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pickle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5YjLXSydn2H",
        "outputId": "47a105db-6bba-4cad-a2e8-7abea5c1f409"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = 'MODEL'\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j1HuEuSdua-",
        "outputId": "67c59324-f37e-45c1-9a97-616bd5883101"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to eliminate consecutive newline characters from a string\n",
        "def eliminate_duplicate_newlines(input_string):\n",
        "    combined = input_string[0]\n",
        "    for character in input_string[1:]:\n",
        "        # Append character if it's not a newline following another newline\n",
        "        if not (character == '\\n' and combined[-1] == '\\n'):\n",
        "            combined += character\n",
        "    return combined\n",
        "\n",
        "# Function to translate TTP (Tactics, Techniques, and Procedures) IDs to their names\n",
        "def translate_ttp_ids_to_names(ttp_identifiers):\n",
        "    # Open the mapping file from TTP ID to TTP Name\n",
        "    with open(ttpid2name, 'rb') as file:\n",
        "        id_to_name_map = pickle.load(file)\n",
        "    # Create a list of names corresponding to the provided TTP identifiers\n",
        "    ttp_names_list = [id_to_name_map[id] for id in ttp_identifiers if id in id_to_name_map]\n",
        "    return ttp_names_list\n",
        "\n",
        "# Function to convert labeled data to TTP IDs\n",
        "def convert_labels_to_ttp_ids(labelled_data):\n",
        "    # Load the mapping from labels to TTP IDs\n",
        "    with open(label2ttpid, 'rb') as file_handle:\n",
        "        label_to_ttpid_map = pickle.load(file_handle)\n",
        "\n",
        "    # Reverse the mapping to get TTP IDs from labels\n",
        "    ttpid_for_labels = {value: key for key, value in label_to_ttpid_map.items()}\n",
        "\n",
        "    # Ensure TTP IDs are unique and convert labels to TTP IDs\n",
        "    unique_ttp_ids = list({ttpid_for_labels[label] for label in labelled_data})\n",
        "\n",
        "    # Fetch names for the unique TTP IDs\n",
        "    names_for_ttp_ids = translate_ttp_ids_to_names(unique_ttp_ids)\n",
        "\n",
        "    return unique_ttp_ids, names_for_ttp_ids\n",
        "\n",
        "# Function to process a text file and extract TTPs using a model and tokenizer\n",
        "def process_text_for_ttps(file_path, prediction_model, text_tokenizer):\n",
        "    technical_ids_predicted = []\n",
        "    processed_sentences = []\n",
        "    # Read and preprocess the text file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_content = file.read()\n",
        "\n",
        "    # Eliminate duplicate newlines and replace tabs and single quotes\n",
        "    document_content = eliminate_duplicate_newlines(document_content)\n",
        "    document_content = document_content.replace('\\t', ' ').replace(\"\\'\", \"'\")\n",
        "\n",
        "    # Split the text into sentences using NLTK\n",
        "    nltk_sentences = nltk.sent_tokenize(document_content)\n",
        "    split_sentences = []\n",
        "    for sentence in nltk_sentences:\n",
        "        split_sentences.extend(sentence.split('\\n'))\n",
        "\n",
        "    # Collect non-empty sentences\n",
        "    for sentence in split_sentences:\n",
        "        if sentence:\n",
        "            processed_sentences.append(sentence)\n",
        "\n",
        "    # Predict TTP IDs for each sentence\n",
        "    for sentence in processed_sentences:\n",
        "        model_inputs = text_tokenizer(sentence, return_tensors=\"pt\")\n",
        "        model_inputs.to(device)\n",
        "\n",
        "        # Use the model to predict the class of each sentence without gradient calculation\n",
        "        with torch.no_grad():\n",
        "            output_logits = prediction_model(**model_inputs).logits\n",
        "\n",
        "        # Determine the class with the highest probability\n",
        "        highest_probability_class_id = output_logits.argmax().item()\n",
        "        # Append the ID if the highest logit exceeds a threshold\n",
        "        if output_logits.amax().cpu() > 6.15:\n",
        "            technical_ids_predicted.append(highest_probability_class_id)\n",
        "\n",
        "    # Convert the predicted technical IDs to TTP identifiers and names\n",
        "    ttp_identifiers_list = convert_labels_to_ttp_ids(technical_ids_predicted)\n",
        "    return ttp_identifiers_list"
      ],
      "metadata": {
        "id": "bvrqci1ruwbd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2ttpid = 'label_dict.pkl'\n",
        "ttpid2name = 'ttp_id_name.pkl'\n",
        "fname= \"APT28_unit42_sofacy_uses_dealerschoice_target_european_government_agency.txt\"\n",
        "ttp_ids, ttp_names = process_text_for_ttps(fname, model, tokenizer)\n",
        "print(\"Extracted TTPs are:\")\n",
        "for i in range(len(ttp_ids)):\n",
        "  print(ttp_ids[i], \" - \", ttp_names[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn9JqZhYgC68",
        "outputId": "2b1aba94-0107-4be0-ebca-9772e5d512c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted TTPs are:\n",
            "T1027  -  Obfuscated Files or Information\n",
            "T1036  -  Masquerading\n",
            "T1071  -  Web Protocols\n",
            "T1090  -  Proxy\n",
            "T1105  -  Ingress Tool Transfer\n",
            "T1132  -  Data Encoding\n",
            "T1204  -  User Execution\n",
            "T1140  -  Deobfuscate/Decode Files or Information\n",
            "T1059  -  Command and Scripting Interpreter\n",
            "T1566  -  Phishing\n",
            "T1053  -  Scheduled Task/Job\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ttp_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcrKCmjKgseb",
        "outputId": "af898f3b-74ab-4335-915e-806e11c1f6e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGv7r4GRmyah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}